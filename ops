{"author":{"id":"382e0604a0cc648df6764230ea7ce0b626217738167eb2200238272b6c52e2a8"},"ops":[{"type":3,"author":{"id":"382e0604a0cc648df6764230ea7ce0b626217738167eb2200238272b6c52e2a8"},"timestamp":1571252215,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU0Mjg0MzQ5Ng==","github-url":"https://github.com/rurban/smhasher/issues/73#issuecomment-542843496"},"nonce":"R5Ygi1y1h3CDix218Dbu2MrEN2A=","message":"@rurban \n\nThat's right. You nailed it.\nI challenge all coders to outspeed FNV1A-Totenschiff, if they do, we all can enjoy a better understanding, function and benchmark experience.\nTo me playing with lookup-table hashers is all about fun and finding higher granularities of that simple code that inherently makes FNV superb.\n\n@dumblob\n\n\u003ee.g. that the performance on mainstream CPUs doesn't matter much or that performance/latency on keys of e.g. 33 or more bytes doesn't matter either\n\nI stand corrected a bit, as always I am a bit marginal, I just hate having a 64bit architecture and be \"awarded\" with penalties for fething with its native order :(\nWhat I really want to see is an English n-gram hasher able to hash in FASTEST mode billions of keys like:\n1-gram: word1\n2-gram: word1_word2\n...\n9-gram: word1_word2_word3_word4_word5_word6_word7_word8_word9\n\nBeing in range 1..100 bytes.\n\n\u003eThen I'm really interested whether FNV1A_Yorikke (or its variants) is at least 1.1x better than XXH3 for keys larger or equal to 3 bytes (smaller keys don't make much sense - you're better off building a fixed-size data structure like an array).\n\nThat's the idea, to throw Yorikke and Totenschiff against best hashers and measure MOSTLY speed and dispersion, for small keys.\nThank you for Yann's bench folder, didn't know of it. I am reluctant to use his bench or hashes just because he could do it as it should, I would probably mess or misuse something. His latest v3 (AFAIU targeting small keys) is in some development stage which adds to that fact - my wish Yann or another professional is to include FNV1A-Totenschiff (targeting small keys) into their bench suites. It would be an interesting showdown.\n\nGuys, thank you for sharing your views, I always enjoy seeing things from different angles, let me tell you what I intend to write at the end of the week (no time atm).\nWill add a simple section before current one that uses each slot for a root of a B-tree, in Lookuperorama r.5 we will have two approaches being run back-to-back - for a given file and a hashtable size, will do one pass to determine Collisions while also reporting the slot containing MAX collisions, after that will run again for each position WYHASH and Totenschiff, 5 times without touching the slots (to avoid random, heh-heh, RAM accesses) - only hashing. Will add resultant hashes to a DUMMY variable, you know. Will report the MIN value of clocks elapsed.\nI intend to do it for all keys in range 1..64, or 64 reporting lines as these, for '1001 Nights':\n\n```\n24bit, RAW Hashing ~15,000,000 keys of size 01 ... [Collisions] [MAX Collisions at some slot] time1 time2 time3 time4 time5 [MIN time in clocks] [Keys-per-second]\n...\n24bit, RAW Hashing ~15,000,000 keys of size 64 ... [Collisions] [MAX Collisions at some slot] time1 time2 time3 time4 time5 [MIN time in clocks] [Keys-per-second]\n```\nIt is 20 minutes away to write it, but I need time also to benchmark Shakespeare, 1001 Nights, CIHAI_Sea-Of-Words, enwik8, also @dumblob in order to amend my mistake I intend to make a roster with three columns:\n\n```\n27bit hashtable, testfile: enwik8 (100,000,000 bytes)\n                -------------------------------------\n                | RAW Hashing Speed, in Keys/s on:  |\n                -------------------------------------\n                | i5-2430M   | i7-3630QM | i5-7200U |\n-----------------------------------------------------\nKeys 01 bytes   | xx,xxx,xxx |\n...             |\nKeys 64 bytes   |\n-----------------------------------------------------\n```","files":null}]}