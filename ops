{"author":{"id":"dea43e6b916975bf3cff4424f473a805c833f29424f086ee359d4eb329d3da27"},"ops":[{"type":3,"author":{"id":"dea43e6b916975bf3cff4424f473a805c833f29424f086ee359d4eb329d3da27"},"timestamp":1571317850,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDU0MzE2NzUyMQ==","github-url":"https://github.com/rurban/smhasher/issues/73#issuecomment-543167521"},"nonce":"IVp6UpPWUm5hpWmHpmxec5peJSQ=","message":"@Sanmayce wrote:\n\n\u003eI am reluctant to use his bench or hashes just because he could do it as it should, I would probably mess or misuse something.\n\nNo, you wouldn't mess anything. It's super easy to use his benchmarks and they're very objective and really **do** test hashing \"at scale\" (I really liked the \"latency\" tests as they're how hashing is being used in majority of cases). By default it uses randomly pre-generated dictionaries, but hey, it's super easy to add arbitrary n-gram dictionaries for many languages (French, German, English, Czech, Polish, Chinese, etc.). I bet it's easier to modify his benchmark then writing your own benchmark (which'll likely have some flaws as benchmarking is **really** hard - see some points in [mera](https://github.com/leo-yuriev/t1ha/blob/cf7d0559842203294fab8e5712bc28c6d1890677/tests/bench.c#L38-L47 ) and in the [blog post](http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html ).\n\nAnd by the way, XXH3 seems functionally pretty stable now, though still without the guarantee of exact same hash outputs across different releases. So don't let yourself go astray and do the fair comparison. We're really eager to see the **comparable** benchmarking results.","files":null}]}