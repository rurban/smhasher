{"author":{"id":"ea8195bf30d85b36cadb8f08e79aa22a50e04d9a05d6d973c74da1c9f099555e"},"ops":[{"type":6,"author":{"id":"ea8195bf30d85b36cadb8f08e79aa22a50e04d9a05d6d973c74da1c9f099555e"},"timestamp":1573641385,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDMwODI4NzUzNA=="},"nonce":"DY3dMTAhsR7SqJTMh/ORWH/nlLw=","target":"1b5c3a385793e5768fe3bddadbd8e6156f513172f727fc8881aa28591ab2c576","message":"Originally I also checked various other much faster linear probing hashmaps, such as e.g. from rigtorp or the two swisstable variants (Google and Facebook), but I settled with the slow standard (seperate chaining) for fair comparison purposes.\nMaybe add other ones, and document them.\n\n@dumblob Speaking about currently worldwide available hashmaps, definitely skim the comparison from Martin Ankerl https://martin.ankerl.com/2019/04/01/hashmap-benchmarks-05-conclusion/ to notice one very important thing. Namely that some hashmaps are almost not at all sensitive to collisions and throughput and latency (Throughput versus latency) of the hash function, but other hashmaps rely completely on the quality and throughput and/or latency of the hash function.\nTo be fair in showing real use case scenarios, we would actually need to test one representative from the sensitive corner and one from the insensitive corner.\n\n@rurban True. There should be a fast, sensitive linear probing also, besides the usual insensitive std chaining. About a robin_hood I'm not sure. It's much more stable than linear probing, and from outside just a fast chaining variant.\nwe need to add the stddev to the mean cycles. I'll also add the initial insert/delete timings, but without trials and outlier removal.\n\nTaken from the discussion at https://github.com/rurban/smhasher/pull/80","files":null},{"type":6,"author":{"id":"ea8195bf30d85b36cadb8f08e79aa22a50e04d9a05d6d973c74da1c9f099555e"},"timestamp":1573641710,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDMwODI5MDY3Mw=="},"nonce":"b8QaDB2gDI5wQKgyKs1gp8wHISg=","target":"1b5c3a385793e5768fe3bddadbd8e6156f513172f727fc8881aa28591ab2c576","message":"Originally I also checked various other much faster linear probing hashmaps, such as e.g. from rigtorp or the two swisstable variants (Google and Facebook), but I settled with the slow standard (seperate chaining) for fair comparison purposes.\nMaybe add other ones, and document them.\n\n@dumblob Speaking about currently worldwide available hashmaps, definitely skim the comparison from Martin Ankerl https://martin.ankerl.com/2019/04/01/hashmap-benchmarks-05-conclusion/ to notice one very important thing. Namely that some hashmaps are almost not at all sensitive to collisions and throughput and latency (Throughput versus latency) of the hash function, but other hashmaps rely completely on the quality and throughput and/or latency of the hash function.\nTo be fair in showing real use case scenarios, we would actually need to test one representative from the sensitive corner and one from the insensitive corner.\n\n@rurban True. There should be a fast, sensitive linear probing also, besides the usual insensitive std chaining. About a robin_hood I'm not sure. It's much more stable than linear probing, and from outside just a fast chaining variant.\nwe need to add the stddev to the mean cycles. I'll also add the initial insert/delete timings, but without trials and outlier removal.\nFor poor hash functions add better security measures, like collision counting or tree-conversion.\nNone of the usual hashmaps are secure, so maybe add fixed versions and test the overhead.\n\nThe goal should be to choose a good hash function, small (inlinable!) and not too many collisions, based on the timings, for slow \u003cunordered_map\u003e and fast hash tables. With string keys, not numbers.\n\nTaken from the discussion at https://github.com/rurban/smhasher/pull/80","files":null},{"type":6,"author":{"id":"ea8195bf30d85b36cadb8f08e79aa22a50e04d9a05d6d973c74da1c9f099555e"},"timestamp":1573641817,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDMwODI5MTY3NQ=="},"nonce":"unPTDUQbiMgTy+BEs9il9IlqDL8=","target":"1b5c3a385793e5768fe3bddadbd8e6156f513172f727fc8881aa28591ab2c576","message":"Originally I also checked various other much faster linear probing hashmaps, such as e.g. from rigtorp or the two swisstable variants (Google and Facebook), but I settled with the slow standard (seperate chaining) for fair comparison purposes.\nMaybe add other ones, and document them.\n\n@dumblob Speaking about currently worldwide available hashmaps, definitely skim the comparison from Martin Ankerl https://martin.ankerl.com/2019/04/01/hashmap-benchmarks-05-conclusion/ to notice one very important thing. Namely that some hashmaps are almost not at all sensitive to collisions and throughput and latency (Throughput versus latency) of the hash function, but other hashmaps rely completely on the quality and throughput and/or latency of the hash function.\nTo be fair in showing real use case scenarios, we would actually need to test one representative from the sensitive corner and one from the insensitive corner.\n\n@rurban True. There should be a fast, sensitive linear probing also, besides the usual insensitive std chaining. About a robin_hood I'm not sure. It's much more stable than linear probing, and from outside just a fast chaining variant.\nwe need to add the stddev to the mean cycles. I'll also add the initial insert/delete timings, but without trials and outlier removal.\nFor poor hash functions add better security measures, like collision counting or tree-conversion.\nNone of the usual hashmaps are secure, so maybe add fixed versions and test the overhead.\n\nThe goal should be to choose a good hash function, small (inlinable!) and not too many collisions, based on the timings, for slow \u003cunordered_map\u003e and fast hash tables. With string keys, not numbers.\n\nParts taken from the discussions at https://github.com/rurban/smhasher/pull/80 and #61","files":null},{"type":5,"author":{"id":"ea8195bf30d85b36cadb8f08e79aa22a50e04d9a05d6d973c74da1c9f099555e"},"timestamp":1573641204,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDI3OTQ3MzIxODU="},"nonce":"EHUXm4WBk/f164FAJA8ZfCsjJig=","added":["enhancement"],"removed":[]}]}