{"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"ops":[{"type":3,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556081847,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDQ4NjA2NzgzMA==","github-url":"https://github.com/rurban/smhasher/issues/65#issuecomment-486067830"},"nonce":"15C8EJ26yaaNT8HeGHN8pnUIwCI=","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1 -\u003e 2, then 2-\u003e 3), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes ? Sounds like a completely arbitrary condition to me.\n\nComparing vs a kind of \"absolute\" distribution would indeed be better,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nA bit like hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here there is none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while bringing no value to the table.\nIn this case, algorithms that ingest seed-size input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying the input or the seed would _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556081890,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxMzc2ODkyNA=="},"nonce":"1BmVuMRPoBVEJpz6O8+ycgSWIvo=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1 -\u003e 2, then 2-\u003e 3), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nA bit like hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here there is none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while bringing no value to the table.\nIn this case, algorithms that ingest seed-size input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying the input or the seed would _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556081945,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxMzc2OTI4MQ=="},"nonce":"cj6oSTVqRZ4yTmQApUtDHucnSNI=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1 -\u003e 2, then 2-\u003e 3), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nA bit like hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here there is none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while bringing no value to the table.\nIn this case, algorithms that ingest seed-size input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying their input or their seed must _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556089036,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxMzgwNDYxNw=="},"nonce":"ncpYPMqfRMYrN6lKPth2EcVLLe0=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1 -\u003e 2, then 2-\u003e 3), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nA bit like hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest seed-size input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying their input or their seed must _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556126599,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDEwNDg5Nw=="},"nonce":"yvegg+5EppP2BIjyrHbEQ4JWWzM=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1*step -\u003e 2*step, then 2*step-\u003e 3*step), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nA bit like hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest seed-size input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying their input or their seed must _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556126641,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDEwNTI2MA=="},"nonce":"/Nj0E5J8wstOCjYQJtgmU4+NRJs=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1*step -\u003e 2*step, then 2*step-\u003e 3*step), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest seed-size input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying their input or their seed must _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556126685,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDEwNTU5Mg=="},"nonce":"oMAZzWn2kVimnIAowZsvRcMbUEc=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1*step -\u003e 2*step, then 2*step-\u003e 3*step), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that produce different series of values depending on modifying their input or their seed must _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556126717,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDEwNTg1NA=="},"nonce":"w7k64m6+/MGp+EOThYgFEH82J/k=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1*step -\u003e 2*step, then 2*step-\u003e 3*step), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556129239,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDEyNTMwNw=="},"nonce":"EVLQpy9Rue6Gbqrh00tpcjS4nO0=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something very strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from 1*step -\u003e 2*step, then 2*step-\u003e 3*step), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556131906,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDE0NDU1NA=="},"nonce":"lNxxpQ0UBncAbMr7W5N2oVf7lHA=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from `1 * step -\u003e 2 * step`, then `2 * step-\u003e 3 * step`), and requires them to be _equal_, or at least feature exactly the same count of important outliers (since they dominate the final evaluation). Who said that a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556131981,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDE0NTAyOQ=="},"nonce":"RbueJDfSymTIcoO4O5I98h7R9Ts=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from `1 * step -\u003e 2 * step`, then `2 * step-\u003e 3 * step`), and requires them to feature exactly the same count of important outliers (since they dominate the final evaluation), which is best achieved by ensuring both series are _equal_. \nWhich should a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556131991,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDE0NTExMw=="},"nonce":"igKENMDAyg6dKbH7e563D2UKuAo=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from `1 * step -\u003e 2 * step`, then `2 * step-\u003e 3 * step`), and requires them to feature exactly the same count of important outliers (since they dominate the final evaluation), which is best achieved by ensuring both series are _equal_. \nWhy should a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce arbitrary restriction which favor some algorithm and blame others while actually bringing no value to the table.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556132083,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDE0NTczNQ=="},"nonce":"H1qmQCerKuPkbG4vpYLb0gTJVCc=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from `1 * step -\u003e 2 * step`, then `2 * step-\u003e 3 * step`), and requires them to feature exactly the same count of important outliers (since they dominate the final evaluation), which is best achieved by ensuring both series are _equal_. \nWhy should a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce some arbitrary restriction which nicely fit some algorithm while blaming others and having actually nothing to do with output quality.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556146571,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDI0NzgxOQ=="},"nonce":"UsLR656jg5BotnBuIqy/WsZ63zw=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from `1 * step -\u003e 2 * step`, then `2 * step-\u003e 3 * step`), and requires them to feature exactly the same count of important outliers (since they dominate the final evaluation), which is best achieved by ensuring that both series are _equal_. \nWhy should a good hash must guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce some arbitrary restriction which nicely fit some algorithm while blaming others and having actually nothing to do with output quality.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null},{"type":6,"author":{"id":"865bf723a9214027b17304fed4f98ae83e229e6421297d65bb8ed683dbeaeb5d"},"timestamp":1556146581,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdDIxNDI0Nzg2NQ=="},"nonce":"6r8f/cIDJrwKP/moE8vMkXTOl3g=","target":"7a61f6f40323cd9c36048043d47f5aec96b1841c945d48bab5b53ae217dca343","message":"This test is requiring something strange :  it compares 2 sets of transitions (since it counts the number of changed bits when input change from `1 * step -\u003e 2 * step`, then `2 * step-\u003e 3 * step`), and requires them to feature exactly the same count of important outliers (since they dominate the final evaluation), which is best achieved by ensuring that both series are _equal_. \nWhy should a good hash guarantee that the serie of transitions when the `seed` changes must be equal to the serie of transitions when the value of `fixed-length-input` changes by the same `step` value ? Sounds like a completely arbitrary condition to me.\n\n__Comparing vs a kind of \"absolute\" distribution would indeed be better__,\nbut then, I find this test does not bring anything valuable compared to the regular distribution test,\nwhich is way more complete in term of exact bit distributions and transitions,\nwhile this one summarizes too much, is more noisy, and driven by extreme outliers.\nIt's somewhat reminiscent of hyperloglog, \nbut hyperloglog features mitigation techniques for outliers, and here I see none.\n\nIt's not enough to add a test with some arbitrary condition and say that \"a good hash must pass it\".\nIt would be too easy to introduce some arbitrary restriction which nicely fit some algorithm while blaming others and having actually nothing to do with output quality.\nFor this specific case, algorithms that ingest 4-bytes input and seed in a commutative operation simply \"game\" this test, because they necessarily produce the same serie, by definition. I don't see why it makes them \"better\", and I don't see why algorithms that just happen to produce different series of values depending on modifying their input or their seed must _necessarily_ be labelled worse.","files":null}]}